use crate::utils::{Bounded, Scale01, Scale100s, Scale22};
use anyhow::{anyhow, Result};
use reqwest::Client;
use serde_json::{json, Value};
use std::collections::HashMap;

use super::{input::Message, output::Body};

pub struct OpenAI {
    pub model: OpenAIModels,
    /// Temperature is used to control the randomness or creativity
    /// of the model's output. Temperature is a parameter that affects
    /// the distribution of probabilities generated by the model.
    pub temperature: Option<f64>,
    /// Limits the length of the generated output.
    /// If `None` it defaults to `Inf`
    pub max_tokens: Option<u64>,
    /// With top_p (probabilistic sampling), the model considers only the most
    /// likely words whose cumulative probability exceeds a specified threshold.
    /// This threshold is determined by the top_p parameter, which is typically
    /// set between 0 and 1.
    ///
    /// A lower value, such as 0.1 or 0.3, restricts the sampling to a
    /// narrower range of high-probability words. This leads to
    /// more focused and deterministic output, with fewer alternatives
    /// and reduced randomness.
    pub top_p: Option<Scale01>,
    /// The frequency penalty parameter helps reduce the repetition of words
    /// or sentences within the generated text. It is a float value ranging
    /// from -2.0 to 2.0, which is subtracted to the logarithmic probability of a
    /// token whenever it appears in the output. By increasing the
    /// frequency penalty value, the model becomes more cautious and less likely
    /// to use repeated tokens frequently.
    ///
    /// In the official documentation:
    /// https://platform.openai.com/docs/api-reference/chat/create#chat/create-frequency_penalty
    pub frequency_penalty: Option<Scale22>,
    /// The presence penalty parameter stears how the model penalizes new tokens
    /// based on whether they have appeared (hence presense) in the text so far.
    ///
    /// The key difference between this param and the frequency param is that the
    /// `presence` penalty is not really concern with the frequency itself.
    ///
    /// In the official documentation:
    /// https://platform.openai.com/docs/api-reference/chat/create#chat/create-presence_penalty
    pub presence_penalty: Option<Scale22>,
    /// How many chat completion choices to generate for each input message.
    pub n: Option<u64>,
    /// Whether to stream back partial progress. If set, tokens will be sent as
    /// data-only server-sent events as they become available, with the stream
    /// terminated by a data: [DONE] message.
    pub stream: bool,
    /// Modify the likelihood of specified tokens appearing in the completion.
    ///
    /// Accepts a json object that maps tokens (specified by their token ID
    /// in the tokenizer) to an associated bias value from -100 to 100.
    /// Mathematically, the bias is added to the logits generated by the model
    /// prior to sampling. The exact effect will vary per model, but values
    /// between -1 and 1 should decrease or increase likelihood of selection;
    /// values like -100 or 100 should result in a ban or exclusive selection
    /// of the relevant token.
    pub logit_bias: HashMap<String, Scale100s>,
    /// A unique identifier representing your end-user, which can help OpenAI
    /// to monitor and detect abuse. You can read more at:
    /// https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids
    pub user: Option<String>,
    api_key: Option<String>,
}

pub enum OpenAIModels {
    Gpt432k,
    Gpt4,
    Gpt35Turbo,
    Gpt35Turbo16k,
}

impl OpenAI {
    pub fn new(model: OpenAIModels) -> Self {
        Self {
            model,
            temperature: None,
            max_tokens: None,
            top_p: None,
            frequency_penalty: None,
            presence_penalty: None,
            n: None,
            stream: false,
            logit_bias: HashMap::new(),
            user: None,
            api_key: None,
        }
    }

    // === Setter methods with chaining ===

    pub fn api_key(mut self, key: String) -> Self {
        self.api_key = Some(key);
        self
    }

    pub fn temperature(mut self, temperature: f64) -> Self {
        self.temperature = Some(temperature);
        self
    }

    pub fn max_tokens(mut self, max_tokens: u64) -> Self {
        self.max_tokens = Some(max_tokens);
        self
    }

    pub fn top_p(mut self, top_p: f64) -> Result<Self> {
        self.top_p = Some(Scale01::new(top_p)?);
        Ok(self)
    }

    pub fn frequency_penalty(mut self, frequency_penalty: f64) -> Result<Self> {
        self.frequency_penalty = Some(Scale22::new(frequency_penalty)?);
        Ok(self)
    }

    pub fn presence_penalty(mut self, presence_penalty: f64) -> Result<Self> {
        self.presence_penalty = Some(Scale22::new(presence_penalty)?);
        Ok(self)
    }

    pub fn n(mut self, n: u64) -> Self {
        self.n = Some(n);
        self
    }

    pub fn stream(mut self, stream: bool) -> Self {
        self.stream = stream;
        self
    }

    pub fn logit_bias(mut self, mut logit_bias: HashMap<String, f64>) -> Result<Self> {
        let logit_bias = logit_bias
            .drain()
            .map(|(key, val)| Ok((key, Scale100s::new(val)?)))
            .collect::<Result<HashMap<String, Scale100s>>>()?;

        self.logit_bias = logit_bias;
        Ok(self)
    }

    // TODO: Add validation
    pub fn user(mut self, user: String) -> Self {
        self.user = Some(user);
        self
    }

    // === OpenAI IO ===

    pub async fn chat(
        &self,
        msgs: &[Message],
        funcs: &[String],
        stop_seq: &[String],
    ) -> Result<Body> {
        let client = Client::new();

        // fill in your own data as needed
        let req_body = self.request_body(msgs, funcs, stop_seq)?;

        let res = client
            .post("https://api.openai.com/v1/chat/completions")
            .header(
                "Authorization",
                format!(
                    "Bearer {}",
                    self.api_key.as_ref().expect("No API Keys provided")
                ),
            )
            .header("Content-Type", "application/json")
            .json(&req_body)
            .send()
            .await?;

        let status = res.status();

        if !status.is_success() {
            return Err(anyhow!("Failed with status: {}", status));
        }

        let body = res.text().await?;
        let api_response = serde_json::from_str(body.as_str())?;

        Ok(api_response)
    }

    fn request_body(
        &self,
        msgs: &[Message],
        funcs: &[String],
        stop_seq: &[String],
    ) -> Result<Value> {
        let mut data = json!({
            "model": self.model.as_str(),
            "messages": msgs,
            // "stop": self.stop,
        });

        if let Some(temperature) = self.temperature {
            data["temperature"] = serde_json::to_value(temperature)?;
        }

        if let Some(top_p) = self.top_p {
            data["top_p"] = serde_json::to_value(top_p)?;
        }

        if !funcs.is_empty() {
            data["functions"] = serde_json::to_value(funcs)?;
        }

        if !stop_seq.is_empty() {
            if stop_seq.len() > 4 {
                return Err(anyhow!(
                    "Maximum limit of stop sequences reached. {} out of 4",
                    stop_seq.len()
                ));
            };

            data["stop"] = serde_json::to_value(stop_seq)?;
        }

        if let Some(user) = &self.user {
            data["user"] = serde_json::to_value(user)?;
        }

        if !self.logit_bias.is_empty() {
            data["logit_bias"] = serde_json::to_value(&self.logit_bias)?;
        }

        if let Some(frequency_penalty) = &self.frequency_penalty {
            data["frequency_penalty"] = serde_json::to_value(frequency_penalty)?;
        }

        if let Some(presence_penalty) = &self.presence_penalty {
            data["presence_penalty"] = serde_json::to_value(presence_penalty)?;
        }

        if let Some(max_tokens) = &self.max_tokens {
            data["max_tokens"] = serde_json::to_value(max_tokens)?;
        }

        if let Some(n) = &self.n {
            data["n"] = serde_json::to_value(n)?;
        }

        Ok(data)
    }
}

impl OpenAIModels {
    pub fn new(model: &str) -> Result<Self> {
        let model = match model {
            "gpt-4-32k" => OpenAIModels::Gpt432k,
            "gpt-4" => OpenAIModels::Gpt4,
            "gpt-3.5-turbo" => OpenAIModels::Gpt35Turbo,
            "gpt-3.5-turbo-16k" => OpenAIModels::Gpt35Turbo16k,
            _ => return Err(anyhow!(format!("Invalid model {}", model))),
        };

        Ok(model)
    }

    pub fn as_str(&self) -> &str {
        match self {
            OpenAIModels::Gpt432k => "gpt-4-32k",
            OpenAIModels::Gpt4 => "gpt-4",
            OpenAIModels::Gpt35Turbo => "gpt-3.5-turbo",
            OpenAIModels::Gpt35Turbo16k => "gpt-3.5-turbo-16k",
        }
    }
}

impl Default for OpenAI {
    fn default() -> Self {
        Self {
            model: OpenAIModels::Gpt35Turbo,
            temperature: None,
            max_tokens: None,
            top_p: None,
            frequency_penalty: None,
            presence_penalty: None,
            n: None,
            stream: false,
            logit_bias: HashMap::new(),
            user: None,
            api_key: None,
        }
    }
}
