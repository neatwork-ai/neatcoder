use anyhow::{anyhow, Result};

struct OpenAI {
    model_name: OpenAIModels,
    /// Temperature is used to control the randomness or creativity
    /// of the model's output. Temperature is a parameter that affects
    /// the distribution of probabilities generated by the model.
    temperature: f64,
    /// Limits the length of the generated output
    max_tokens: u64,
    /// With top_p (probabilistic sampling), the model considers only the most
    /// likely words whose cumulative probability exceeds a specified threshold.
    /// This threshold is determined by the top_p parameter, which is typically
    /// set between 0 and 1.
    ///
    /// A lower value, such as 0.1 or 0.3, restricts the sampling to a
    /// narrower range of high-probability words. This leads to
    /// more focused and deterministic output, with fewer alternatives
    /// and reduced randomness.
    top_p: Probability,
    /// The frequency penalty parameter aims to reduce the repetition of words
    /// or sentences within the generated text. It is a float value ranging
    /// from -2.0 to 2.0, which is subtracted to the logarithmic probability of a
    /// token whenever it appears in the output. By increasing the
    /// frequency penalty value, the model becomes more cautious and less likely
    /// to use repeated tokens frequently.
    ///
    /// In the official documentation:
    /// https://platform.openai.com/docs/api-reference/chat/create#chat/create-frequency_penalty
    frequency_penalty: f64,
}

struct Probability(f32);

impl Probability {
    pub fn new(prob: f32) -> Result<Probability> {
        if prob < 0.0 || prob > 1.0 {
            return Err(anyhow!(format!("Probability can't be {}", prob)));
        }

        Ok(Probability(prob))
    }
}

enum OpenAIModels {
    DaVinci003,
}
