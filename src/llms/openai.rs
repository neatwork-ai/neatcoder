use crate::utils::{Scale01, Scale100s, Scale22};
use anyhow::{anyhow, Result};
use std::collections::HashMap;

// TODO: Validation: best_of must be > n

struct OpenAI {
    model_name: OpenAIModels,
    /// Temperature is used to control the randomness or creativity
    /// of the model's output. Temperature is a parameter that affects
    /// the distribution of probabilities generated by the model.
    temperature: f64,
    /// Limits the length of the generated output
    max_tokens: u64,
    /// With top_p (probabilistic sampling), the model considers only the most
    /// likely words whose cumulative probability exceeds a specified threshold.
    /// This threshold is determined by the top_p parameter, which is typically
    /// set between 0 and 1.
    ///
    /// A lower value, such as 0.1 or 0.3, restricts the sampling to a
    /// narrower range of high-probability words. This leads to
    /// more focused and deterministic output, with fewer alternatives
    /// and reduced randomness.
    top_p: Scale01,
    /// The frequency penalty parameter helps reduce the repetition of words
    /// or sentences within the generated text. It is a float value ranging
    /// from -2.0 to 2.0, which is subtracted to the logarithmic probability of a
    /// token whenever it appears in the output. By increasing the
    /// frequency penalty value, the model becomes more cautious and less likely
    /// to use repeated tokens frequently.
    ///
    /// In the official documentation:
    /// https://platform.openai.com/docs/api-reference/chat/create#chat/create-frequency_penalty
    frequency_penalty: Scale22,
    /// The presence penalty parameter stears how the model penalizes new tokens
    /// based on whether they have appeared (hence presense) in the text so far.
    ///
    /// The key difference between this param and the frequency param is that the
    /// `presence` penalty is not really concern with the frequency itself.
    ///
    /// In the official documentation:
    /// https://platform.openai.com/docs/api-reference/chat/create#chat/create-presence_penalty
    presence_penalry: Scale22,
    /// How many chat completion choices to generate for each input message.
    n: u64,
    /// Generates best_of completions server-side and returns the best (the one
    /// with the highest log probability per token). Results cannot be streamed.
    ///
    /// When used with n, best_of controls the number of candidate completions
    /// and `n` specifies how many to return â€“ `best_of` must be greater than `n`.
    ///
    /// Note: Because this parameter generates many completions, it can quickly
    /// consume your token quota. Use carefully and ensure that you have
    /// reasonable settings for `max_tokens` and `stop`.
    ///
    /// In the official documentation:
    /// https://platform.openai.com/docs/api-reference/completions/create#completions/create-best_of
    best_of: u64,
    /// Whether to stream back partial progress. If set, tokens will be sent as
    /// data-only server-sent events as they become available, with the stream
    /// terminated by a data: [DONE] message.
    strem: bool,
    /// Modify the likelihood of specified tokens appearing in the completion.
    ///
    /// Accepts a json object that maps tokens (specified by their token ID
    /// in the tokenizer) to an associated bias value from -100 to 100.
    /// Mathematically, the bias is added to the logits generated by the model
    /// prior to sampling. The exact effect will vary per model, but values
    /// between -1 and 1 should decrease or increase likelihood of selection;
    /// values like -100 or 100 should result in a ban or exclusive selection
    /// of the relevant token.
    logit_bias: HashMap<String, Scale100s>,
}

enum OpenAIModels {
    DaVinci003,
}
